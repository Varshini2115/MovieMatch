# -*- coding: utf-8 -*-
"""Movie Recommender System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17L0OPFlC4iBPe9wCWQj-BZ-uBJtGtdI-

# Importing Required Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import operator
from collections import Counter
import warnings
import json
from keras.models import Model
from keras.layers import Input, Dense, Embedding, concatenate, Flatten, Activation, Add, Dropout, Multiply
from keras.optimizers import Adam

from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
import warnings
warnings.filterwarnings('ignore')

"""# Lets look into each dataset provided

### Credits
"""

credits = pd.read_csv('credits.csv')
credits.head(5)

"""### Keywords"""

keywords = pd.read_csv('keywords.csv')
keywords.head(5)

"""### Links"""

links = pd.read_csv('links.csv')
links.head(5)

"""### Links Small"""

links_small = pd.read_csv('links_small.csv')

links_small.head(5)

"""Both the Links small and Links datasets contain identical information, with the Links small dataset being a subset of the Links dataset. Consequently, we can disregard either one.

### Ratings Small
"""

ratings_small = pd.read_csv('ratings_small.csv')
ratings_small.head(5)

"""Similarly, we have both the Ratings and Ratings small datasets, where one is a subset of the other. Therefore, we can choose to disregard one of them.

### Movies Metadata
"""

movies = pd.read_csv('movies_metadata.csv')
movies.head(5)

print(f'Number of rows: {movies.shape[0]}')
print(f'Number of columns: {movies.shape[1]}')

"""### Dropping irrelevant columns"""

movies.drop(movies[movies['adult'] == 'True'].index, axis='rows', inplace=True)
movies.drop(labels = ['adult', 'belongs_to_collection', 'homepage', 'poster_path', 'video'], axis='columns', inplace=True)

"""We will be working with the remaning columns shown below. As the columns were all over the place, rearranging them with the reindex method makes it look more organized. You can compare the DataFrame outputs from above and below to see if this made any difference."""

movies.columns
movies = movies.reindex(columns = ['imdb_id','title','original_title','release_date','overview','tagline','genres', 'runtime',
                                   'original_language','spoken_languages','production_companies','production_countries',
                                   'budget', 'revenue','status',  'vote_average','vote_count','popularity',])
movies.head(3)

"""### Data Cleaning

The columns such as *genres*, *production_companies*, *production_countries*, and *spoken_languages* appear to be JSON objects converted into strings. The code cells below extract the necessary substrings using regular expressions.
"""

movies[['title', 'genres', 'production_companies', 'production_countries', 'spoken_languages']].head(3)

"""From the displayed columns, it's evident that the desired substring adheres to a specific pattern: a colon, followed by whitespace, a single quote, one or more words, and finally another single quote. We can encapsulate this pattern with a regular expression, compile it, and utilize it to locate all instances of words matching this pattern in each column through the application of a lambda function."""

import re
regex = re.compile(r": '(.*?)'")
movies['genres'] = movies['genres'].apply(lambda x: ', '.join(regex.findall(x)))

"""We can apply the same code to the remaining columns - *production_companies*, *production_countries*, and *spoken_languages*. However, it's necessary to remove missing values to simplify the lambda function and avoid additional logic. Since the number of missing values is insignificant compared to the entire dataset, dropping them wouldn't impact subsequent analysis."""

print('Number of missing values in production_companies column: {}'.format(movies['production_companies'].isna().sum()))
print('Number of missing values in spoken_languages column: {}'.format(movies['spoken_languages'].isna().sum()))

movies.dropna(subset=['production_companies'], axis='rows', inplace=True)
movies.dropna(subset=['spoken_languages'], axis='rows', inplace=True)

print('Number of missing values in production_companies column: {}'.format(movies['production_companies'].isna().sum()))
print('Number of missing values in spoken_languages column: {}'.format(movies['spoken_languages'].isna().sum()))

movies['production_companies'] = movies['production_companies'].apply(lambda x: ', '.join(regex.findall(x)))
movies['production_countries'] = movies['production_countries'].apply(lambda x: ', '.join(regex.findall(x)))
movies['spoken_languages'] = movies['spoken_languages'].apply(lambda x: ', '.join(regex.findall(x)))

movies[['title', 'genres', 'production_companies', 'production_countries', 'spoken_languages']].head(3)

"""### Changing Columns to Correct Dtypes"""

movies.info()

movies['budget'] = movies['budget'].astype(float)
movies['popularity'] = movies['popularity'].astype(float)
movies['release_date'] = pd.to_datetime(movies['release_date'], format='%Y/%m/%d', errors='coerce')
movies['runtime'] = pd.to_timedelta(movies['runtime'], unit='m')

movies.info()

"""### Dropping Missing values"""

print('Number of missing values in imdb_id: {}'.format(movies['imdb_id'].isna().sum()))
movies.dropna(subset=['imdb_id'], inplace=True)
print('Number of missing values in imdb_id after drop: {}'.format(movies['imdb_id'].isna().sum()))

"""### Removing Duplicates"""

cond = movies['imdb_id'].duplicated(keep=False)
movies.loc[cond, ['imdb_id','title','release_date', 'overview']].sort_values('imdb_id').head(10)

print('Number of duplicate imdb_ids before drop: {}'.format(movies['imdb_id'].duplicated().sum()))
movies.drop_duplicates('imdb_id', inplace=True)
print('Number of duplicate imdb_ids remaining: {}'.format(movies['imdb_id'].duplicated().sum()))

cond = movies['title'].duplicated(keep=False)
movies.loc[cond, ['imdb_id','title','release_date', 'overview']].sort_values('title').head(10)

"""### Data Exploration and Visualization

### Summary Statistics
"""

movies.describe().T

runtime_int = movies['runtime']/np.timedelta64(1, 'm')

with plt.style.context('seaborn'):
    fig = plt.figure(figsize=(20,20));
    grid = plt.GridSpec(3, 2, wspace=0.2, hspace=0.2);
    plt.rc(('xtick', 'ytick'), labelsize=15); plt.rc('axes', labelsize=15); plt.rcParams["patch.force_edgecolor"] = True;
    _ = plt.subplot(grid[0, 0:]); _ = sns.distplot(runtime_int, kde=False, axlabel='Runtime in Minutes');
    _ = plt.subplot(grid[1,0]); _ = sns.distplot(movies['budget'], kde=False, axlabel='Budget in USD Millions');
    _ = plt.subplot(grid[1,1]); _ = sns.distplot(movies['revenue'], kde=False, axlabel='Revenue in USD Billions');
    _ = plt.subplot(grid[2,0]); _ = sns.distplot(movies['vote_average'], kde=False, axlabel='Vote Average');
    _ = plt.subplot(grid[2,1]); _ = sns.distplot(movies['vote_count'], kde=False, axlabel='Vote Count');

"""The histograms represent the data displayed above. It's evident that all histograms, except one, exhibit significant right skewness, primarily because of a large number of zero or small values. The prevalent assumption for zero values is likely due to data unavailability, as it's improbable to produce a movie for free without generating at least some revenue.

Among the columns, only Vote_average closely resembles a normal distribution, with a median of 6.

### Top 10 <a id='C60'></a>

Listed below are the top 10 movies based on budget, revenue, hits, flops and ratings.

#### Budget <a id='C61'></a>
"""

cols = ['title', 'budget']
budget_df = movies.sort_values('budget', ascending=False)[cols].set_index('title')
top_10_budget = budget_df.head(10)
sns.set_style('dark')
sns.barplot(data=top_10_budget, x=top_10_budget.index, y='budget');
plt.xticks(ha='left', rotation=-20, fontsize=15); plt.yticks(fontsize=15)
plt.xlabel(''); plt.ylabel('USD 100 Million', fontsize=15);
plt.title('Top 10 Highest Budget Movies', fontsize=15);
plt.show()

"""#### Box Office Revenue <a id='C64'></a>"""

cols = ['title', 'revenue']
revenue_df = movies.sort_values('revenue', ascending=False)[cols].set_index('title')
top_10_revenue = revenue_df.head(10)

fig, ax = plt.subplots(figsize=(15,5))
sns.set_style('dark')
sns.barplot(data=top_10_revenue, x=top_10_revenue.index, y='revenue');
plt.xticks(ha='left', rotation=-20, fontsize=15); plt.yticks(fontsize=15)
plt.xlabel(''); plt.ylabel('USD Billion', fontsize=15);
plt.title('Top 10 Highest Revenue Movies', fontsize=15);
plt.show()

"""Regarding profit percentage, Avatar maintains its position at the top. However, the rankings shift for other movies. It's also astounding to observe how successful movies achieve profits that far exceed their initial budgets. For instance, Avatar has generated over 10 times its budget, while Avengers: Age of Ultron, which ranks lowest on this list, has earned nearly 4 times its budget.

#### Box Office Hits <a id='C67'></a>
"""

profits_ser = movies['revenue'] - movies['budget']
profits_ser.name = 'profit'
profits_df = movies.join(profits_ser)[['title', 'budget', 'revenue', 'profit']].sort_values('profit', ascending=False)
top_10_profits = profits_df.head(10).set_index('title')

plt.style.use('ggplot')
top_10_profits.plot(kind='bar', figsize=(20,4), fontsize=20)
plt.ylabel('USD Billion', fontsize=20); plt.xlabel('')
plt.xticks(rotation=-20, ha='left')
plt.suptitle('Budget, Revenue and Profit for the Top 10 Profitable Movies', fontsize=20)
plt.show()

profits_ser_perc = (top_10_profits['profit'] / top_10_profits['budget'] * 100)
profits_ser_perc = profits_ser_perc.sort_values(ascending=False).to_frame().rename(columns={0:'Profit Percentage'})

"""The visual representation clearly illustrates the substantial gap between the budget and profit among the top ten most profitable movies."""

fig, ax = plt.subplots(figsize=(15,5))
sns.set_style('dark')
sns.barplot(data=profits_ser_perc, x=profits_ser_perc.index, y='Profit Percentage')
plt.xticks(ha='left', rotation=-20, fontsize=15); plt.yticks(fontsize=15)
plt.xlabel(''); plt.ylabel('Profit Percentage', fontsize=15);
plt.title('Profit in Percentage for the Top 10 Profitable Movies', fontsize=15);
plt.show()

"""When considering profit percentage, the narrative shifts slightly. While Avatar retains its top position, the rankings vary for other movies. It's truly remarkable how successful films manage to generate profits that surpass their initial budgets. For instance, Avatar has earned over 10 times its budget, while Avengers: Age of Ultron, situated at the bottom of this list, has yielded nearly 4 times its budget.

#### Box Office Flops <a id='C73'></a>
"""

top_10_loss = profits_df[profits_df['revenue'] > 0].tail(10).sort_values(['profit', 'revenue']).set_index('title')

plt.style.use('seaborn')
top_10_loss.plot(kind='bar', figsize=(20,6), fontsize=20)
plt.ylabel('USD Billion', fontsize=20); plt.xlabel('')
plt.xticks(rotation=-20, ha='left')
plt.suptitle('Top 10 Losing Movies', fontsize=20)
plt.show()

"""I'm somewhat taken aback to see The Lone Ranger at the top of this list. Although I haven't seen the movie myself, I was aware that Johnny Depp starred in it, and I assumed it was a success. The 13th Warrior is another unexpected inclusion. In my experience, movies with compelling plots tend to have IMDb ratings greater than 6. It's interesting to note that both of these movies have IMDb ratings higher than 6 but haven't performed well at the box office. As for the other movies on this list, I don't have much insight into them.

##### Cleaning Credits Dataframe

Let's extract only the cast and the director from the credits DataFrame for now.
"""

import re

cast_regex = re.compile(r"'name': '(.*?)'")
director_regex = re.compile(r"'Director', 'name': '(.*?)'")
credits['cast'] = credits['cast'].apply(lambda x: ', '.join(cast_regex.findall(x)))
credits['director'] = credits['crew'].apply(lambda x: ', '.join(director_regex.findall(x)))

credits.head()

"""##### Merging Directors and Movie Genres
Listed below are directors and the movies genres for each movie id.
"""

cond = credits.director == ''
directors = credits.loc[~cond, ['id', 'director']] #List of directors without ''

directors.head()

cond2 = movies.genres == ''
genres = movies.loc[~cond2, 'genres'].reset_index() #Movie genres without ''

genres.head()

genres.rename(columns={'index':'id'},inplace=True)

genres['id'] = genres['id'].astype(int) #Changing id to from obj to int for merging

director_genre = pd.merge(genres, directors, on='id') #Merging
director_genre.head(5)

print(directors)

"""##### Creating Dummy Varaibles for Genres
This process will help with retrieving the totals of each genre that a director has worked in.
"""

genre_dummies = director_genre.genres.str.get_dummies(sep=', ') #Creating dummy variables for genres from director_genre
director_genre_dummies = director_genre.join(genre_dummies) #Joining director_genre with genre_dummies
director_genre_dummies.head(3)

"""##### Famous Directors Genre Totals
Let us now create genre totals for all directors from the above DataFrame and filter only famous directors.
"""

famous_directors = ['Martin Scorsese', 'Quentin Tarantino', 'Steven Spielberg', 'Alfred Hitchcock', 'Christopher Nolan',
                   'Tim Burton', 'James Cameron', 'Ridley Scott', 'George Lucas', 'Woody Allen', 'Clint Eastwood',
                   'Michael Bay', 'Guillermo del Toro', 'John Carpenter', 'Oliver Stone', 'Anurag Kashyap',
                   'Satyajit Ray', 'Mani Ratnam', 'Yash Chopra', 'Rajkumar Hirani', 'Prakash Jha', 'Karan Johar',
                   'S. Shankar', 'Mahesh Bhatt', 'Imtiaz Ali', 'A.R. Murugadoss'] # a small list of famous directors(google)

director_genre_totals = director_genre_dummies.groupby('director').sum() #summing the genres for each director with groupby
cond = director_genre_totals.index.isin(famous_directors) #checking if famous director are in director_genre_totals
famous_director_genres = director_genre_totals[cond] #filter for famous directors
famous_director_genres.drop('id', axis='columns', inplace=True) #dropping id as it is not required
famous_director_genres.head(3)

"""### Merge the two data frames"""

#concat the two dataframes
movies_concat = pd.concat([credits, movies], axis=1)
movies_concat.head()

#remove the duplicated column 'id'
movies_df = movies_concat.loc[:,~movies_concat.columns.duplicated()]
movies_df.head()

"""## Demographic Filtering

Before we proceed:

1. We need a metric to score or rate movies.
2. The score for each movie must be calculated.
3. The best-rated movie according to users' preferences needs to be identified and scored.

For this purpose, we're using the IMDB's weighted rating (wr) formula:
![image.png](attachment:image.png)
Where:
- \( v \) is the number of votes for the movie.
- \( m \) is the minimum votes required to be listed in the chart.
- \( R \) is the average rating of the movie.
- \( C \) is the mean vote across the whole report.
v (vote_count) and
R (vote_average) are already given, and
C can be calculated as:
"""

C= movies_df['vote_average'].mean()
C

"""The mean rating for all movies is approximately 6 on a scale of 10.

Next, we need to determine an appropriate value for \( m \), the minimum votes required to be listed in the chart. The cutoff is set to 0.9, meaning that for a movie to be featured in the charts, it must have more votes than at least 90% of the movies in the list.
"""

m= movies_df['vote_count'].quantile(0.9)
m

"""The next step is to filter out the movies that qualify for the chart"""

q_movies = movies_df.copy().loc[movies_df['vote_count'] >= m]
q_movies.shape

"""Now, we need to calculate the metric for each qualified movie."""

def weighted_rating(x, m=m, C=C):
    v = x['vote_count']
    R = x['vote_average']
    # Calculation based on the IMDB formula
    return (v/(v+m) * R) + (m/(m+v) * C)

# Define a new feature 'score' and calculate its value with `weighted_rating()`
q_movies['score'] = q_movies.apply(weighted_rating, axis=1)

#Sort movies based on score calculated above
q_movies = q_movies.sort_values('score', ascending=False)

#Print the top 15 movies
q_movies[['title', 'vote_count', 'vote_average', 'score']].head(10)

#plot the popularity of the movies
pop= movies_df.sort_values('popularity', ascending=False)
import matplotlib.pyplot as plt
plt.figure(figsize=(12,4))

plt.barh(pop['title'].head(6),pop['popularity'].head(6), align='center',
        color='skyblue')
plt.gca().invert_yaxis()
plt.xlabel("Popularity")
plt.title("Popular Movies")

plt.show()

"""### Content Based Filtering

In this recommender system, the content of the movie, including its overview, cast, crew, keywords, tagline, etc., is utilized to determine its similarity with other movies. By analyzing these attributes, the system identifies movies that are most likely to be similar and recommends them to users.

![image.png](attachment:image.png)

### Plot description based Recommender

We'll calculate pairwise similarity scores for all movies based on their plot descriptions and recommend movies based on those similarity scores. The plot descriptions are provided in the overview feature of our dataset. Let's examine the data.
"""

movies_df['overview'].head(5)

#Import TfIdfVectorizer from scikit-learn
from sklearn.feature_extraction.text import TfidfVectorizer

#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'
tfidf = TfidfVectorizer(stop_words='english')

#Replace NaN with an empty string
movies_df['overview'] = movies_df['overview'].fillna('')

#Construct the required TF-IDF matrix by fitting and transforming the data
tfidf_matrix = tfidf.fit_transform(movies_df['overview'])

#Output the shape of tfidf_matrix
tfidf_matrix.shape

"""![image.png](attachment:image.png)"""

# Import linear_kernel
from sklearn.metrics.pairwise import linear_kernel

from sklearn.neighbors import NearestNeighbors

# Fit the NearestNeighbors model
nn_model = NearestNeighbors(metric='cosine', algorithm='brute')
nn_model.fit(tfidf_matrix)

# Compute the cosine similarity matrix
distances, indices = nn_model.kneighbors(tfidf_matrix)
cosine_sim = 1 - distances

#Construct a reverse map of indices and movie titles
indices = pd.Series(movies_df.index, index=movies_df['title']).drop_duplicates()

"""To define our recommendation function, we'll follow these steps:

1. Obtain the index of the movie given its title.
2. Retrieve the list of cosine similarity scores for that specific movie with all other movies. Convert this list into a list of tuples where the first element is its position and the second is the similarity score.
3. Sort the list of tuples based on the similarity scores (the second element).
4. Obtain the top 10 elements from this sorted list. Exclude the first element as it refers to the movie itself (the movie most similar to a particular movie is the movie itself).
5. Return the titles corresponding to the indices of the top elements.
"""

# Function that takes in movie title as input and outputs most similar movies
def get_recommendations(title, cosine_sim=cosine_sim):
    # Get the index of the movie that matches the title
    idx = indices[title]

    # Get the pairwsie similarity scores of all movies with that movie
    sim_scores = list(enumerate(cosine_sim[idx]))

    # Sort the movies based on the similarity scores
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Get the scores of the 10 most similar movies
    sim_scores = sim_scores[1:11]

    # Get the movie indices
    movie_indices = [i[0] for i in sim_scores]

    # Return the top 10 most similar movies
    return movies_df['title'].iloc[movie_indices]

get_recommendations('The Dark Knight Rises') #test recommendation for the movie 'The Dark Knight Rises'

get_recommendations('Interstellar')  #test recommendation for the movie 'Interstellar'

"""### Collaborative Filtering

In this section, we'll employ a technique called Collaborative Filtering to provide recommendations to Movie Watchers. It consists of two main types:

1. **User-based filtering:** These systems suggest products to a user that similar users have liked. We can measure the similarity between two users using Pearson correlation or cosine similarity.

2. **Item-based Collaborative Filtering:** Instead of measuring the similarity between users, item-based CF recommends items based on their similarity with the items that the target user rated. Similarly, we can compute similarity using Pearson Correlation or Cosine Similarity. The primary distinction is that with item-based collaborative filtering, we fill in the blanks vertically, as opposed to the horizontal manner in which user-based CF operates. The following table demonstrates how to do so for the movie "Me Before You."
"""

!pip install scikit-surprise

from surprise import Reader, Dataset, SVD
from surprise.model_selection import cross_validate
reader=Reader()

ratings_small.head()

data = Dataset.load_from_df(ratings_small[['userId', 'movieId', 'rating']], reader)

svd=SVD()
cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)

trainset = data.build_full_trainset()
svd.fit(trainset)

ratings_small[ratings_small['userId'] == 1]

svd.predict(1, 302, 3)

"""For the movie with ID 302, the estimated prediction is 2.766. One notable aspect of this recommender system is that it doesn't consider the content of the movie (or its features). It operates solely based on an assigned movie ID and aims to predict ratings by analyzing how other users have rated the movie.

## Collaborative based Recommendation System using KNN
"""

from sklearn.neighbors import NearestNeighbors

links  = links.merge(ratings_small, on = 'movieId')

links.head(5)

movies.columns

links.columns

movies_meta=pd.read_csv("movies_metadata.csv",usecols=['id','imdb_id','title'])

movies_meta.columns

links.rename(columns = {'tmdbId':'id'}, inplace = True)
movies_meta = movies_meta.drop([29503,35587,19730])
movies_meta['id'] = movies_meta['id'].astype('int64')
movies_meta = movies_meta.merge(links,on='id')

df = movies_meta

df.head(5)

df = movies_meta[['userId', 'movieId', 'rating', 'title']]

df.head(5)

combine_movie_rating = df.dropna(axis = 0, subset = ['title'])

movie_ratingCount = (df.
                     groupby(by = ['title'])['rating'].
                     count().
                     reset_index().
                     rename(columns = {'rating': 'totalRatingCount'})
                     [['title', 'totalRatingCount']])

movie_ratingCount.head()

rating_with_totalRatingCount = combine_movie_rating.merge(movie_ratingCount, left_on = 'title', right_on = 'title', how = 'left')

rating_with_totalRatingCount.head()

pd.set_option('display.float_format', lambda x: '%.3f' % x)
print(movie_ratingCount['totalRatingCount'].describe())

"""### Lets assume that if the popularity count is greater than 50 (more than 50 ratings) its score should be higher and to be recommended"""

popularity_threshold = 50
rating_popular_movie= rating_with_totalRatingCount.query('totalRatingCount >= @popularity_threshold')
rating_popular_movie.head()

movie_features_df = rating_popular_movie.pivot_table(index='title', columns='userId', values='rating').fillna(0)
movie_features_df.head(10)

"""![image.png](attachment:image.png)

### csr_matrix

When a matrix consists mostly of 0 values, it's termed a sparse matrix. Representing such a matrix with a 2D array results in a significant waste of memory because the zeroes hold no meaningful information in many cases. Therefore, instead of storing all elements, we only store the non-zero elements. This involves storing non-zero elements as triples, with each triple representing (Row, Column, Value).
"""

from scipy.sparse import csr_matrix

# converting the matrix into array matrix
movie_features_df_matrix = csr_matrix(movie_features_df.values)


from sklearn.neighbors import NearestNeighbors

model_knn = NearestNeighbors(metric = 'cosine', algorithm = 'brute')

model_knn.fit(movie_features_df_matrix)

print(movie_features_df_matrix)

def recommend_movie(movie_name):

    distances, indices = model_knn.kneighbors(movie_features_df.loc[movie_name,:].values.reshape(1, -1), n_neighbors = 6)

    for i in range(0, len(distances.flatten())):
        if i == 0:
            #print('Recommendations for {0}:\n'.format(movie_features_df.index[indices.flatten()[0]]))
            print('Recommendations for {0}:\n'.format(movie_name))
        else:
            print('{0}: {1}, with distance of {2}:'.format(i, movie_features_df.index[indices.flatten()[i]], distances.flatten()[i]))

recommend_movie('A Close Shave')

recommend_movie('Avatar')

"""### NEURAL COLLABORATIVE FILTER"""

ratings_small.sort_values('timestamp')

from sklearn.preprocessing import LabelEncoder
user_encoder = LabelEncoder()
movie_encoder = LabelEncoder()

user_ids = user_encoder.fit_transform(ratings_small.userId)
movie_ids = movie_encoder.fit_transform(ratings_small.movieId)

# train / val split
num_train = int(len(user_ids) * 0.8)
train_user_ids = user_ids[:num_train]
train_movie_ids = movie_ids[:num_train]
train_ratings = ratings_small.rating.values[:num_train]
val_user_ids = user_ids[num_train:]
val_movie_ids = movie_ids[num_train:]
val_ratings = ratings_small.rating.values[num_train:]

num_users= user_ids.max()+1
num_movies = movie_ids.max() + 1

train_ratings /= 5
val_ratings /= 5

def get_ncf_model():
    user_inp = Input((1,))
    user_hidden = Embedding(input_dim=num_users, output_dim=64)(user_inp)
    user_hidden = Flatten()(user_hidden)

    item_inp = Input((1,))
    item_hidden = Embedding(input_dim=num_movies, output_dim=64)(item_inp)
    item_hidden = Flatten()(item_hidden)

    # element-wise multiplication
    mf_output = Multiply()([user_hidden, item_hidden])

    hidden = concatenate([user_hidden, item_hidden])
    hidden = Dense(128, activation='relu')(hidden)
    hidden = Dropout(0.2)(hidden)
    mlp_output = Dense(64, activation='relu')(hidden)


    output = concatenate([mf_output, mlp_output])
    output = Dense(1, activation='sigmoid')(output)

    model = Model(inputs=[user_inp, item_inp], outputs=output)
    model.compile(loss='mse', optimizer='adam')
    return model

model = get_ncf_model()
model.summary()

from keras.callbacks import EarlyStopping
# early stopping wait for 1 epoch
callbacks = [EarlyStopping(patience=1)]

# train for 50 epochs
model.fit([train_user_ids, train_movie_ids], train_ratings,\
          validation_data=([val_user_ids, val_movie_ids], val_ratings), epochs=50, batch_size=128, callbacks=callbacks)

def dcg_at_k(r, k):
    '''
    Compute DCG
    args:
        r: np.array, to be evaluated
        k: int, number of entries to be considered

    returns:
        dcg: float, computed dcg

    '''
    r = r[:k]
    dcg = np.sum(r / np.log2(np.arange(2, len(r) + 2)))
    return dcg

def ndcg_at_k(r, k, method=0):
    '''
    Compute NDCG
    args:
        r: np.array, to be evaluated
        k: int, number of entries to be considered

    returns:
        dcg: float, computed ndcg

    '''
    dcg_max = dcg_at_k(sorted(r, reverse=True), k)

    return dcg_at_k(r, k) / dcg_max

# compute average ndcg for all users
def evaluate_prediction(predictions):
    '''
    Return the average ndcg for each users
    args:
        predictions: np.array user-item predictions
    returns:
        ndcg: float, computed NDCG
    '''
    ndcgs = []
    for target_user in np.unique(val_user_ids):
        # get movie ids and ratings associated with the target user.
        target_val_movie_ids = val_movie_ids[val_user_ids == target_user]
        target_val_ratings = val_ratings[val_user_ids == target_user]

        # compute ndcg for this user
        ndcg = ndcg_at_k(target_val_ratings[np.argsort(-predictions[val_user_ids == target_user])], k=30)
        ndcgs.append(ndcg)
    ndcg = np.mean(ndcgs)
    return ndcg

# prediction & evalutation
predictions = model.predict([val_user_ids, val_movie_ids])
evaluate_prediction(predictions[:,0])